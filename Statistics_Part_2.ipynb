{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Statistics Part 2\n"
      ],
      "metadata": {
        "id": "njDtVfjpbPqq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q1. What is hypothesis testing in statistics ?\n",
        "Hypothesis testing is a statistical method used to make decisions or inferences about a population based on sample data. It helps you determine whether a claim or assumption about a population parameter is likely to be true.\n",
        "\n",
        "Here’s how it works in a nutshell:\n",
        "\n",
        "1. **Formulate two hypotheses**:\n",
        "   - **Null hypothesis (H₀)**: This is the default assumption—usually that there is no effect or no difference.  \n",
        "   - **Alternative hypothesis (H₁ or Ha)**: This is what you want to test for—suggesting there is an effect or a difference.\n",
        "\n",
        "2. **Choose a significance level (α)**: Commonly set at 0.05, this represents the probability of rejecting the null hypothesis when it’s actually true (Type I error).\n",
        "\n",
        "3. **Collect and analyze sample data**: You calculate a test statistic (like a z-score or t-score) based on your data.\n",
        "\n",
        "4. **Compare the p-value to α**:\n",
        "   - If **p-value ≤ α**, reject the null hypothesis (evidence supports the alternative).\n",
        "   - If **p-value > α**, fail to reject the null hypothesis (not enough evidence to support the alternative).\n",
        "\n",
        "5. **Draw a conclusion**: Based on the comparison, you decide whether your data supports the claim you're testing.\n",
        "\n",
        "There are different types of tests—**one-tailed** (testing for a specific direction of effect) and **two-tailed** (testing for any difference, regardless of direction).\n",
        "\n"
      ],
      "metadata": {
        "id": "Fi4Koc7-bcKE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q2. What is the null hypothesis, and how does it differ from the alternative hypothesis\n",
        "\n",
        " Understanding the difference between the **null hypothesis** and the **alternative hypothesis** is key to mastering hypothesis testing.\n",
        "\n",
        "### Null Hypothesis (H₀)\n",
        "This is the default assumption or status quo. It states that **there is no effect, no difference, or no relationship** between variables. Think of it as the claim you're trying to test _against_.\n",
        "\n",
        "Example:  \n",
        "> “There is no difference in average test scores between students who study with music and those who don’t.”\n",
        "\n",
        "### Alternative Hypothesis (H₁ or Ha)\n",
        "This is the claim you’re trying to find evidence for. It suggests that **there is an effect, a difference, or a relationship**.\n",
        "\n",
        "Example:  \n",
        "> “Students who study with music score differently (higher or lower) than those who don’t.”\n",
        "\n",
        "### Key Differences\n",
        "| Feature | Null Hypothesis (H₀) | Alternative Hypothesis (H₁) |\n",
        "|--------|----------------------|-----------------------------|\n",
        "| **Meaning** | No effect or difference | Some effect or difference |\n",
        "| **Goal** | Try to disprove or reject | Try to support |\n",
        "| **Assumed True Until...** | Evidence suggests otherwise | Evidence supports it |\n",
        "| **Symbol** | H₀ | H₁ or Ha |\n",
        "\n",
        "In practice, we never “prove” the alternative—we just gather enough evidence to **reject the null**. It’s a bit like a courtroom: the null is “innocent until proven guilty.”\n",
        "\n"
      ],
      "metadata": {
        "id": "RM1ltyMGb5qd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q3.  What is the significance level in hypothesis testing, and why is it important ?\n",
        "\n",
        "The **significance level**, often denoted by **α (alpha)**, is the threshold you set in hypothesis testing to decide whether to reject the null hypothesis. It represents the **maximum probability of making a Type I error**—that is, rejecting the null hypothesis when it’s actually true.\n",
        "\n",
        "### Why it matters:\n",
        "- **Controls false positives**: A lower α (like 0.01) means you're being more cautious about claiming a result is statistically significant.\n",
        "- **Sets the bar for evidence**: It defines how strong your sample evidence must be to confidently say, “This result probably didn’t happen by chance.”\n",
        "- **Common choices**: Researchers often use α = 0.05, meaning they’re willing to accept a 5% chance of being wrong when rejecting the null.\n",
        "\n",
        "### Real-world analogy:\n",
        "Think of it like a courtroom. The significance level is your standard of proof—how sure you need to be before declaring someone guilty. A stricter α is like requiring more convincing evidence.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BzplGprZcq63"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q4.What does a P-value represent in hypothesis testing ?\n",
        "The **p-value** in hypothesis testing tells you how likely it is to observe your sample results—or something more extreme—**if the null hypothesis were true**.\n",
        "\n",
        "### In simple terms:\n",
        "It answers the question: _“Assuming there’s no real effect, how surprising is this data?”_\n",
        "\n",
        "- A **small p-value** (typically ≤ 0.05) means your data is **unlikely under the null hypothesis**, so you might reject the null.\n",
        "- A **large p-value** suggests your data is **consistent with the null**, so you don’t have strong evidence to reject it.\n",
        "\n",
        "### Example:\n",
        "Let’s say you’re testing whether a new teaching method improves student scores.  \n",
        "- **H₀**: The method has no effect.  \n",
        "- You run a test and get a **p-value of 0.02**.  \n",
        "- That means there’s a **2% chance** of seeing results this extreme if the method truly had no effect.  \n",
        "- Since 0.02 < 0.05, you’d likely reject H₀ and say the method might be effective.\n",
        "\n",
        "It’s important to remember: **a p-value doesn’t tell you the probability that the null hypothesis is true**—just how compatible your data is with it.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RBASK7fEdCKm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q5. How do you interpret the P-value in hypothesis testing ?\n",
        "Interpreting the **p-value** is all about understanding how surprising your data is—*assuming the null hypothesis is true*.\n",
        "\n",
        "### Here's how to think about it:\n",
        "\n",
        "- A **small p-value** (typically ≤ 0.05) means your data is **unlikely under the null hypothesis**. This gives you reason to **reject the null** and consider the alternative.\n",
        "- A **large p-value** suggests your data is **plausible under the null**, so you **fail to reject** it.\n",
        "\n",
        "### Example:\n",
        "Let’s say you’re testing whether a new drug lowers blood pressure:\n",
        "- **H₀**: The drug has no effect.\n",
        "- You get a **p-value of 0.03**.\n",
        "- That means there’s a **3% chance** of observing your results (or more extreme ones) if the drug truly had no effect.\n",
        "\n",
        "Since 0.03 < 0.05, you’d reject H₀ and say the drug likely has an effect.\n",
        "\n",
        "### Important notes:\n",
        "- A **p-value is not** the probability that the null hypothesis is true.\n",
        "- It also doesn’t measure the size or importance of an effect—just the strength of evidence *against* H₀.\n",
        "\n"
      ],
      "metadata": {
        "id": "1FzzuPDgdjfo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q5. How do you interpret the P-value in hypothesis testing ?\n",
        "The **P-value** is like a reality check for your hypothesis. It tells you how surprising your data is *if* the null hypothesis were actually true.\n",
        "\n",
        "Here’s the breakdown:\n",
        "\n",
        "- A **small P-value** (typically ≤ 0.05) means your data is *unlikely* under the null hypothesis. That gives you reason to **reject** the null — your results are statistically significant.\n",
        "- A **large P-value** (> 0.05) means your data is *plausible* under the null. So, you **fail to reject** the null — no strong evidence for the alternative.\n",
        "\n",
        "Think of it like this: if you flip a coin 100 times and get 90 heads, the P-value tells you how likely that is if the coin were fair. A tiny P-value would suggest the coin might be rigged.\n",
        "\n",
        "It’s important to remember: the P-value doesn’t tell you the probability that the null is true — it tells you how compatible your data is with the null.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ddW77TjIMxzy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q6. What are Type 1 and Type 2 errors in hypothesis testing ?\n",
        "In hypothesis testing, **Type I and Type II errors** are the two classic mistakes you can make when drawing conclusions from data. Think of them as the statistical version of “false alarms” and “missed signals.”\n",
        "\n",
        "---\n",
        "\n",
        "### 🔹 Type I Error (False Positive)\n",
        "This happens when you **reject the null hypothesis (H₀) even though it’s actually true**.\n",
        "\n",
        "- **Analogy**: You think a fire alarm is going off because there’s a fire—but there isn’t.\n",
        "- **Example**: A medical test says a patient has a disease when they actually don’t.\n",
        "- **Probability of this error**: Denoted by **α (alpha)**, often set at 0.05.\n",
        "\n",
        "---\n",
        "\n",
        "### 🔹 Type II Error (False Negative)\n",
        "This occurs when you **fail to reject the null hypothesis when it’s actually false**.\n",
        "\n",
        "- **Analogy**: There’s a fire, but the alarm doesn’t go off.\n",
        "- **Example**: A test fails to detect a disease that the patient actually has.\n",
        "- **Probability of this error**: Denoted by **β (beta)**.\n",
        "\n",
        "---\n",
        "\n",
        "### 🧠 Quick Comparison\n",
        "\n",
        "| Error Type | What Happens | Real-World Analogy | Controlled By |\n",
        "|------------|--------------|--------------------|----------------|\n",
        "| **Type I** | Rejecting a true H₀ | False alarm | Significance level (α) |\n",
        "| **Type II** | Not rejecting a false H₀ | Missed detection | Power of the test (1 - β) |\n",
        "\n",
        "---\n",
        "\n",
        "Both errors are important to consider when designing experiments. Reducing one often increases the other, so it’s all about finding the right balance.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uKX1_JoGd9eX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q7. What is the difference between a one-tailed and a two-tailed test in hypothesis testing\n",
        " The difference between **one-tailed** and **two-tailed** tests lies in the direction of the effect you're testing for in your hypothesis.\n",
        "\n",
        "---\n",
        "\n",
        "### 🔹 One-Tailed Test\n",
        "This test checks for an effect in **only one direction**—either greater than or less than a certain value.\n",
        "\n",
        "- **Alternative Hypothesis (H₁)**: The parameter is either **greater than** or **less than** the null value.\n",
        "- **Use case**: When you have a strong reason to expect a specific direction.\n",
        "- **Example**:  \n",
        "  H₀: μ = 50  \n",
        "  H₁: μ > 50 (right-tailed) → you're only interested if the mean is *greater* than 50.\n",
        "\n",
        "---\n",
        "\n",
        "### 🔸 Two-Tailed Test\n",
        "This test checks for an effect in **both directions**—whether the parameter is **different** (either higher or lower) from the null value.\n",
        "\n",
        "- **Alternative Hypothesis (H₁)**: The parameter is **not equal to** the null value.\n",
        "- **Use case**: When you're open to detecting a difference in **either direction**.\n",
        "- **Example**:  \n",
        "  H₀: μ = 50  \n",
        "  H₁: μ ≠ 50 → you're testing if the mean is *either* higher or lower than 50.\n",
        "\n",
        "---\n",
        "\n",
        "### 🧠 Quick Comparison\n",
        "\n",
        "| Feature | One-Tailed Test | Two-Tailed Test |\n",
        "|--------|------------------|------------------|\n",
        "| **Direction** | One (left or right) | Both |\n",
        "| **H₁ Symbol** | > or < | ≠ |\n",
        "| **Critical Region** | One tail of the distribution | Both tails |\n",
        "| **When to Use** | Specific directional claim | Any difference |\n",
        "\n",
        "---\n",
        "\n",
        "Choosing the right test depends on your research question. If you're testing whether a new teaching method improves scores (and only care about improvement), a one-tailed test might fit. But if you're checking whether it changes scores in *any* way, go two-tailed.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PQ3qsT5fecGU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q8. What is the Z-test, and when is it used in hypothesis testing ?\n",
        "The **Z-test** is a statistical method used in hypothesis testing to determine whether there's a significant difference between sample and population means—or between two sample means—**when the population standard deviation is known** and the sample size is large (typically **n > 30**).\n",
        "\n",
        "---\n",
        "\n",
        "### 🧪 When to Use a Z-Test:\n",
        "- The data is **approximately normally distributed**.\n",
        "- The **population standard deviation (σ)** is known.\n",
        "- The **sample size is large** (n > 30).\n",
        "- You're testing **means** or **proportions**.\n",
        "\n",
        "---\n",
        "\n",
        "### 🔹 Common Types of Z-Tests:\n",
        "1. **One-sample Z-test**:  \n",
        "   Tests whether the mean of a single sample differs from a known population mean.  \n",
        "   Example: Is the average battery life of a new phone model different from the advertised 12 hours?\n",
        "\n",
        "2. **Two-sample Z-test**:  \n",
        "   Compares the means of two independent samples to see if they differ significantly.  \n",
        "   Example: Do students from two different schools have different average test scores?\n",
        "\n",
        "3. **Z-test for proportions**:  \n",
        "   Used when comparing sample proportions to a population proportion or between two groups.\n",
        "\n",
        "---\n",
        "\n",
        "### 🧮 Z-Test Formula:\n",
        "For a one-sample Z-test:\n",
        "\\[\n",
        "Z = \\frac{\\bar{x} - \\mu}{\\sigma / \\sqrt{n}}\n",
        "\\]\n",
        "Where:  \n",
        "- \\(\\bar{x}\\) = sample mean  \n",
        "- \\(\\mu\\) = population mean  \n",
        "- \\(\\sigma\\) = population standard deviation  \n",
        "- \\(n\\) = sample size\n",
        "\n",
        "---\n",
        "\n",
        "### 🧠 Why it’s useful:\n",
        "The Z-test leverages the **standard normal distribution** to calculate how far your sample statistic is from the population parameter in terms of standard deviations. If the result falls in the critical region (based on your α level), you reject the null hypothesis.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fbl-Rx9Ve4sI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q9. How do you calculate the Z-score, and what does it represent in hypothesis testing\n",
        "The **Z-score** (or **standard score**) tells you how many standard deviations a data point or sample mean is from the population mean. In hypothesis testing, it helps you determine whether your observed result is statistically significant under the assumption that the null hypothesis is true.\n",
        "\n",
        "---\n",
        "\n",
        "### 🧮 Z-Score Formula (for a sample mean):\n",
        "\\[\n",
        "Z = \\frac{\\bar{x} - \\mu}{\\sigma / \\sqrt{n}}\n",
        "\\]\n",
        "\n",
        "Where:  \n",
        "- \\(\\bar{x}\\) = sample mean  \n",
        "- \\(\\mu\\) = population mean  \n",
        "- \\(\\sigma\\) = population standard deviation  \n",
        "- \\(n\\) = sample size\n",
        "\n",
        "---\n",
        "\n",
        "### 🔍 What It Represents:\n",
        "- A **Z-score of 0** means your sample mean is exactly equal to the population mean.\n",
        "- A **positive Z-score** means the sample mean is above the population mean.\n",
        "- A **negative Z-score** means it’s below.\n",
        "- The **larger the absolute value**, the more unusual the result is under the null hypothesis.\n",
        "\n",
        "---\n",
        "\n",
        "### 🧠 In Hypothesis Testing:\n",
        "Once you calculate the Z-score, you compare it to a **critical value** (based on your significance level α) or use it to find a **p-value**. If the Z-score falls in the critical region (e.g., beyond ±1.96 for α = 0.05 in a two-tailed test), you reject the null hypothesis.\n",
        "\n"
      ],
      "metadata": {
        "id": "dCj80JicfWYp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q10. What is the T-distribution, and when should it be used instead of the normal distribution ?\n",
        "The **t-distribution** (or **Student’s t-distribution**) is a probability distribution that’s similar to the normal distribution but has **heavier tails**—meaning it’s more prone to producing values that fall far from its mean. This makes it especially useful when dealing with **small sample sizes** or **unknown population standard deviations**.\n",
        "\n",
        "---\n",
        "\n",
        "### 🧪 When to Use the T-Distribution:\n",
        "Use the t-distribution **instead of the normal distribution** when:\n",
        "- The **sample size is small** (typically **n ≤ 30**).\n",
        "- The **population standard deviation (σ)** is **unknown**.\n",
        "- The data is **approximately normally distributed**.\n",
        "\n",
        "---\n",
        "\n",
        "### 🔍 Why It Matters:\n",
        "When sample sizes are small, we have more uncertainty about the population parameters. The t-distribution accounts for this by spreading out more (wider tails), which leads to **more conservative estimates**—like wider confidence intervals or larger critical values.\n",
        "\n",
        "As the sample size increases, the t-distribution **approaches the normal distribution**. So for large samples, the difference becomes negligible.\n",
        "\n",
        "---\n",
        "\n",
        "### 🧠 Quick Comparison:\n",
        "\n",
        "| Feature | Normal Distribution | T-Distribution |\n",
        "|--------|----------------------|----------------|\n",
        "| Shape | Bell-shaped, thinner tails | Bell-shaped, heavier tails |\n",
        "| Use When | σ is known, large n | σ is unknown, small n |\n",
        "| Critical Values | Smaller | Larger (more conservative) |\n",
        "| Degrees of Freedom | Not needed | Required (n - 1) |\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "s5DlO2mxfxkC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q11. What is the difference between a Z-test and a T-test .\n",
        "The **Z-test** and **T-test** are both used in hypothesis testing to compare means, but they differ in when and how they’re applied. Here's a clear breakdown:\n",
        "\n",
        "---\n",
        "\n",
        "### 🔍 **Key Differences**\n",
        "\n",
        "| Feature | **Z-Test** | **T-Test** |\n",
        "|--------|------------|------------|\n",
        "| **When to Use** | Large sample size (**n ≥ 30**) | Small sample size (**n < 30**) |\n",
        "| **Population Standard Deviation (σ)** | **Known** | **Unknown** |\n",
        "| **Distribution Used** | Standard **normal distribution** | **t-distribution** (heavier tails) |\n",
        "| **Test Statistic** | Z-score | t-score |\n",
        "| **Degrees of Freedom** | Not required | Required (usually **n - 1**) |\n",
        "\n",
        "---\n",
        "\n",
        "### 🧠 Why It Matters:\n",
        "- The **Z-test** is more precise when σ is known and the sample is large—it assumes less uncertainty.\n",
        "- The **T-test** is more flexible and conservative, especially with small samples or when σ is unknown—it accounts for extra variability.\n",
        "\n",
        "---\n",
        "\n",
        "### 🧪 Real-World Analogy:\n",
        "Imagine you're estimating the average height of students:\n",
        "- If you know the **exact population standard deviation** and have data from **100 students**, use a **Z-test**.\n",
        "- If you're working with **just 15 students** and don’t know σ, go with a **T-test**.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "CDJNpJbTgRHS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q12. What is the T-test, and how is it used in hypothesis testing ?\n",
        "The **t-test** is a statistical method used in hypothesis testing to determine whether there's a **significant difference between the means of two groups**—especially when the **sample size is small** and the **population standard deviation is unknown**.\n",
        "\n",
        "---\n",
        "\n",
        "### 🔍 What It Does:\n",
        "It tests whether the observed difference between sample means (or between a sample mean and a known value) is likely due to chance, or if it's statistically meaningful.\n",
        "\n",
        "---\n",
        "\n",
        "### 🧪 How It’s Used in Hypothesis Testing:\n",
        "\n",
        "1. **Set up hypotheses**  \n",
        "   - **Null hypothesis (H₀)**: There is no difference between the means.  \n",
        "   - **Alternative hypothesis (H₁)**: There is a difference.\n",
        "\n",
        "2. **Choose the type of t-test**  \n",
        "   - **One-sample t-test**: Compare a sample mean to a known value.  \n",
        "   - **Independent two-sample t-test**: Compare means of two independent groups.  \n",
        "   - **Paired sample t-test**: Compare means from the same group at different times (e.g., before and after treatment).\n",
        "\n",
        "3. **Calculate the t-statistic**  \n",
        "   It measures how far your sample result is from the null hypothesis, in terms of standard error.\n",
        "\n",
        "4. **Determine the p-value**  \n",
        "   Based on the t-distribution and degrees of freedom, you find the probability of observing such a result if H₀ were true.\n",
        "\n",
        "5. **Make a decision**  \n",
        "   - If **p-value ≤ α** (e.g., 0.05), reject H₀ → significant difference.  \n",
        "   - If **p-value > α**, fail to reject H₀ → no significant difference.\n",
        "\n",
        "---\n",
        "\n",
        "### 📌 Example:\n",
        "Suppose you want to test if a new teaching method improves test scores. You collect scores from 20 students before and after using the method. A **paired t-test** can help you determine if the improvement is statistically significant.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "Zoty7cxAgtX2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q13. What is the relationship between Z-test and T-test in hypothesis testing ?\n",
        "The **Z-test** and **T-test** are closely related—they’re both used to test hypotheses about population means—but they differ mainly in the assumptions they make and the situations in which they’re used.\n",
        "\n",
        "---\n",
        "\n",
        "### 🔗 **Their Relationship at a Glance**\n",
        "\n",
        "| Aspect | **Z-Test** | **T-Test** |\n",
        "|--------|------------|------------|\n",
        "| **Used When** | Sample size is **large** (n ≥ 30) | Sample size is **small** (n < 30) |\n",
        "| **Population Standard Deviation (σ)** | **Known** | **Unknown** |\n",
        "| **Distribution** | Standard **normal distribution** | **t-distribution** (with heavier tails) |\n",
        "| **Precision** | More precise with large samples | More conservative with small samples |\n",
        "| **Converges to Z** | — | As sample size increases, **t-distribution approaches normal** |\n",
        "\n",
        "---\n",
        "\n",
        "### 🧠 How They’re Connected:\n",
        "- Both tests compare sample data to a population parameter to assess statistical significance.\n",
        "- The **t-test is essentially a generalization of the Z-test**—it’s used when you don’t know the population standard deviation and must estimate it from the sample.\n",
        "- As your sample size grows, the **t-distribution becomes nearly identical to the normal distribution**, so the **t-test and Z-test give nearly the same results**.\n",
        "\n",
        "---\n",
        "\n",
        "### 🎯 In Practice:\n",
        "If you know σ and have a large sample, use a **Z-test**.  \n",
        "If σ is unknown or your sample is small, go with a **T-test**.\n",
        "\n"
      ],
      "metadata": {
        "id": "Lbkb8pNAhLxA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q14. What is a confidence interval, and how is it used to interpret statistical results ?\n",
        "A **confidence interval (CI)** is a range of values that’s used to estimate an unknown population parameter—like a mean or proportion—based on sample data. Instead of giving a single number (a point estimate), it gives a **range** that likely contains the true value, along with a **confidence level** that quantifies how sure we are.\n",
        "\n",
        "---\n",
        "\n",
        "### 🔍 What It Means:\n",
        "If you calculate a **95% confidence interval** for the average height of students and get **160–170 cm**, it means:\n",
        "\n",
        "> “We are 95% confident that the true average height of all students lies between 160 and 170 cm.”\n",
        "\n",
        "This doesn’t mean there’s a 95% chance the true value is in that range—it means that if you repeated the sampling process many times, **95 out of 100 intervals** would contain the true value.\n",
        "\n",
        "---\n",
        "\n",
        "### 🧮 Formula (for a mean):\n",
        "\\[\n",
        "\\text{Confidence Interval} = \\bar{x} \\pm (\\text{Critical Value} \\times \\text{Standard Error})\n",
        "\\]\n",
        "Where:\n",
        "- \\(\\bar{x}\\) = sample mean  \n",
        "- Critical Value = from Z or t-distribution (based on confidence level)  \n",
        "- Standard Error = \\(\\frac{\\text{Standard Deviation}}{\\sqrt{n}}\\)\n",
        "\n",
        "---\n",
        "\n",
        "### 📌 Why It’s Useful:\n",
        "- **Quantifies uncertainty** in estimates\n",
        "- Helps assess **reliability** of results\n",
        "- Widely used in **A/B testing**, **survey analysis**, and **machine learning**\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "wYJd1sVJhfLD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q15. What is the margin of error, and how does it affect the confidence interval ?\n",
        "The **margin of error** is the amount you're allowing for uncertainty in your estimate—it's like a buffer zone around your sample statistic that accounts for sampling variability.\n",
        "\n",
        "---\n",
        "\n",
        "### 🔍 What It Represents:\n",
        "In a confidence interval, the margin of error defines **how far above or below** the sample estimate the true population parameter might be.\n",
        "\n",
        "For example, if a survey finds that 60% of people support a policy with a **margin of error of ±3%**, the confidence interval is **57% to 63%**. That means you're reasonably confident the true support lies within that range.\n",
        "\n",
        "---\n",
        "\n",
        "### 🧮 How It’s Calculated:\n",
        "\\[\n",
        "\\text{Margin of Error} = \\text{Critical Value} \\times \\text{Standard Error}\n",
        "\\]\n",
        "\n",
        "- **Critical Value**: Based on your confidence level (e.g., 1.96 for 95% confidence using a Z-distribution).\n",
        "- **Standard Error**: Reflects the variability in your sample.\n",
        "\n",
        "---\n",
        "\n",
        "### 🎯 How It Affects the Confidence Interval:\n",
        "The **confidence interval** is built around your sample statistic like this:\n",
        "\\[\n",
        "\\text{Confidence Interval} = \\text{Sample Estimate} \\pm \\text{Margin of Error}\n",
        "\\]\n",
        "\n",
        "So, a **larger margin of error** means a **wider interval**—more uncertainty. A **smaller margin of error** means a **narrower interval**—more precision.\n",
        "\n",
        "---\n",
        "\n",
        "### 🧠 What Influences the Margin of Error:\n",
        "- **Sample size**: Larger samples → smaller margin of error.\n",
        "- **Confidence level**: Higher confidence (like 99%) → larger margin of error.\n",
        "- **Population variability**: More variability → larger margin of error.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "hDbzOsLph2VL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q16. How is Bayes' Theorem used in statistics, and what is its significance ?\n",
        "Bayes’ Theorem is like a statistical compass—it helps you **update your beliefs** when new evidence comes in. It’s a cornerstone of **Bayesian inference**, which is all about refining probabilities as you learn more.\n",
        "\n",
        "---\n",
        "\n",
        "### 🔁 The Formula:\n",
        "\\[\n",
        "P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)}\n",
        "\\]\n",
        "\n",
        "Where:\n",
        "- **P(A)** = Prior probability (your belief before seeing the data)\n",
        "- **P(B|A)** = Likelihood (how likely the evidence is if A is true)\n",
        "- **P(B)** = Marginal probability of the evidence\n",
        "- **P(A|B)** = Posterior probability (your updated belief after seeing the data)\n",
        "\n",
        "---\n",
        "\n",
        "### 🧠 Why It’s Significant:\n",
        "- **Incorporates prior knowledge**: Unlike classical (frequentist) methods, Bayes’ Theorem lets you start with an assumption and refine it.\n",
        "- **Adapts with new data**: It’s dynamic—perfect for real-world situations where information evolves.\n",
        "- **Used in many fields**: From **medical diagnostics** (e.g., updating disease probability after a test result) to **machine learning**, **spam filtering**, and **decision-making under uncertainty**.\n",
        "\n",
        "---\n",
        "\n",
        "### 📌 Real-World Example:\n",
        "Suppose a disease affects 1% of a population. A test is 99% accurate. If someone tests positive, what’s the chance they actually have the disease?  \n",
        "Bayes’ Theorem helps you combine the **base rate** (1%) with the **test accuracy** to get a more realistic answer—often much lower than you'd expect.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "6GsRLMihiJR8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q17. What is the Chi-square distribution, and when is it used ?\n",
        "The **Chi-square (χ²) distribution** is a continuous probability distribution that arises when you sum the squares of independent standard normal variables. In simpler terms, if you take several values from a standard normal distribution, square them, and add them up—you get a Chi-square distributed value.\n",
        "\n",
        "---\n",
        "\n",
        "### 🔍 What It Represents:\n",
        "If \\( Z_1, Z_2, ..., Z_k \\) are independent standard normal variables, then:\n",
        "\\[\n",
        "\\chi^2 = Z_1^2 + Z_2^2 + \\dots + Z_k^2\n",
        "\\]\n",
        "This sum follows a Chi-square distribution with **k degrees of freedom** (df), where *k* is the number of variables.\n",
        "\n",
        "---\n",
        "\n",
        "### 🧪 When It’s Used:\n",
        "The Chi-square distribution is a workhorse in **hypothesis testing**, especially for **categorical data**. Here are the most common applications:\n",
        "\n",
        "1. **Chi-square test of independence**  \n",
        "   - Checks if two categorical variables are related.  \n",
        "   - Example: Is there a relationship between gender and voting preference?\n",
        "\n",
        "2. **Chi-square goodness-of-fit test**  \n",
        "   - Tests whether observed frequencies match expected frequencies.  \n",
        "   - Example: Do dice rolls follow a uniform distribution?\n",
        "\n",
        "3. **Test for population variance**  \n",
        "   - When the underlying population is normal, you can use the Chi-square distribution to test hypotheses about variance.\n",
        "\n",
        "---\n",
        "\n",
        "### 📌 Key Properties:\n",
        "- **Non-negative**: Values are always ≥ 0.\n",
        "- **Right-skewed**: Especially with fewer degrees of freedom.\n",
        "- **As df increases**: The distribution becomes more symmetric and approaches a normal distribution.\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YQ1aVlZMiifh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q18. What is the Chi-square goodness of fit test, and how is it applied ?\n",
        "The **Chi-square goodness of fit test** is a statistical method used to determine whether the distribution of a **categorical variable** in your sample matches an expected distribution. In other words, it helps answer the question: _“Does what I observed differ significantly from what I expected?”_\n",
        "\n",
        "---\n",
        "\n",
        "### 🧪 When to Use It:\n",
        "- You have **one categorical variable** with two or more levels (e.g., colors, brands, preferences).\n",
        "- You want to test whether the observed frequencies match a **theoretical or expected distribution**.\n",
        "\n",
        "---\n",
        "\n",
        "### 🧮 The Formula:\n",
        "\\[\n",
        "\\chi^2 = \\sum \\frac{(O_i - E_i)^2}{E_i}\n",
        "\\]\n",
        "Where:\n",
        "- \\(O_i\\) = observed frequency for category *i*  \n",
        "- \\(E_i\\) = expected frequency for category *i*\n",
        "\n",
        "---\n",
        "\n",
        "### 🧠 Hypotheses:\n",
        "- **Null hypothesis (H₀)**: The observed distribution matches the expected distribution.\n",
        "- **Alternative hypothesis (H₁)**: The observed distribution does not match the expected distribution.\n",
        "\n",
        "---\n",
        "\n",
        "### 📌 Example:\n",
        "Suppose a company claims that customers are equally likely to choose one of three new dog food flavors. You test this with 75 dogs and observe:\n",
        "\n",
        "- Flavor A: 30  \n",
        "- Flavor B: 20  \n",
        "- Flavor C: 25  \n",
        "\n",
        "Expected frequency for each (if equally likely) = 75 ÷ 3 = 25\n",
        "\n",
        "You’d plug these into the formula to calculate the Chi-square statistic, then compare it to a critical value (or use a p-value) to decide whether to reject H₀.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "-_UVPDi9i7eZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q19.What is the F-distribution, and when is it used in hypothesis testing ?\n",
        "The **F-distribution** is a continuous probability distribution that arises frequently in statistics, especially when comparing **variances** or evaluating **multiple group means**. It’s shaped like a skewed bell curve and is defined by two parameters: the **degrees of freedom** for the numerator and denominator.\n",
        "\n",
        "---\n",
        "\n",
        "### 🔍 What It Represents:\n",
        "If you take two independent chi-square variables (each divided by their respective degrees of freedom) and form a ratio, that ratio follows an F-distribution. Mathematically:\n",
        "\\[\n",
        "F = \\frac{(S_1^2 / \\text{df}_1)}{(S_2^2 / \\text{df}_2)}\n",
        "\\]\n",
        "Where \\( S_1^2 \\) and \\( S_2^2 \\) are sample variances.\n",
        "\n",
        "---\n",
        "\n",
        "### 🧪 When It’s Used in Hypothesis Testing:\n",
        "\n",
        "1. **Analysis of Variance (ANOVA)**  \n",
        "   - To test whether **three or more group means** are significantly different.  \n",
        "   - Example: Comparing test scores across four different teaching methods.\n",
        "\n",
        "2. **Comparing Two Variances**  \n",
        "   - To test if the **variances of two populations** are equal.  \n",
        "   - Example: Checking if two machines produce items with the same consistency.\n",
        "\n",
        "3. **Regression Analysis**  \n",
        "   - To test the **overall significance** of a regression model.  \n",
        "   - It helps determine if your independent variables explain a significant portion of the variance in the dependent variable.\n",
        "\n",
        "---\n",
        "\n",
        "### 📌 Key Characteristics:\n",
        "- **Always non-negative** (F ≥ 0)\n",
        "- **Right-skewed**, especially with small degrees of freedom\n",
        "- As degrees of freedom increase, it becomes more symmetric\n",
        "\n",
        "---\n",
        "\n",
        "The F-distribution is like the referee in a statistical match—it helps you decide whether the differences you see are just noise or something meaningful.\n",
        "\n"
      ],
      "metadata": {
        "id": "w46jZengjXnF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q20. What is an ANOVA test, and what are its assumptions ?\n",
        "The **ANOVA test** (Analysis of Variance) is a statistical method used to determine whether there are **significant differences between the means of three or more independent groups**. Instead of comparing means pairwise (like multiple t-tests), ANOVA evaluates all groups simultaneously, helping control the risk of Type I errors.\n",
        "\n",
        "---\n",
        "\n",
        "### 🧪 When to Use ANOVA:\n",
        "- You have **one continuous dependent variable** (e.g., test scores).\n",
        "- You have **one or more categorical independent variables** (e.g., teaching methods, diet types).\n",
        "- You want to test if **group means differ significantly**.\n",
        "\n",
        "---\n",
        "\n",
        "### 🔍 Key Assumptions of ANOVA:\n",
        "\n",
        "1. **Independence of observations**  \n",
        "   - Each data point should be collected independently.  \n",
        "   - For example, one person’s test score shouldn’t influence another’s.\n",
        "\n",
        "2. **Normality**  \n",
        "   - The data in each group should be **approximately normally distributed**.  \n",
        "   - This can be checked using Q-Q plots or tests like Shapiro-Wilk.\n",
        "\n",
        "3. **Homogeneity of variances (homoscedasticity)**  \n",
        "   - The **variances across groups should be roughly equal**.  \n",
        "   - You can test this with **Levene’s test** or **Bartlett’s test**.\n",
        "\n",
        "---\n",
        "\n",
        "### 🧠 Why These Assumptions Matter:\n",
        "Violating them can lead to **misleading results**. For example, if variances are unequal, ANOVA might detect a difference that’s not really there—or miss one that is.\n",
        "\n",
        "If assumptions are violated, you might consider:\n",
        "- **Transforming the data**\n",
        "- Using **Welch’s ANOVA** (for unequal variances)\n",
        "- Switching to **non-parametric tests** like the Kruskal-Wallis test\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KDu740VvjvaO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q21. What are the different types of ANOVA tests?\n",
        "There are several types of **ANOVA (Analysis of Variance)** tests, each designed for different experimental setups and research questions. Here's a breakdown of the most common ones:\n",
        "\n",
        "---\n",
        "\n",
        "### 🔹 **One-Way ANOVA**\n",
        "- **Purpose**: Compares the means of **three or more groups** based on **one independent variable**.\n",
        "- **Example**: Testing whether different fertilizers affect plant growth differently.\n",
        "\n",
        "---\n",
        "\n",
        "### 🔸 **Two-Way ANOVA**\n",
        "- **Purpose**: Examines the effect of **two independent variables** on a dependent variable, and whether there’s an **interaction** between them.\n",
        "- **Example**: Studying how both teaching method and class time affect student performance.\n",
        "\n",
        "---\n",
        "\n",
        "### 🔁 **Repeated Measures ANOVA**\n",
        "- **Purpose**: Used when the **same subjects** are measured **multiple times** under different conditions or time points.\n",
        "- **Example**: Measuring blood pressure of patients before, during, and after treatment.\n",
        "\n",
        "---\n",
        "\n",
        "### 🧩 **Factorial ANOVA**\n",
        "- **Purpose**: A generalization of two-way ANOVA that handles **more than two factors**, each with multiple levels.\n",
        "- **Example**: Analyzing the effects of diet, exercise, and sleep on weight loss.\n",
        "\n",
        "---\n",
        "\n",
        "### 🎯 **MANOVA (Multivariate ANOVA)**\n",
        "- **Purpose**: Extends ANOVA when there are **multiple dependent variables**.\n",
        "- **Example**: Testing how different therapies affect both anxiety and depression scores.\n",
        "\n",
        "---\n",
        "\n",
        "Each type of ANOVA helps you answer slightly different questions about group differences and interactions.\n"
      ],
      "metadata": {
        "id": "vojfqG0OkGxI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q22. What is the F-test, and how does it relate to hypothesis testing?\n",
        "The **F-test** is a statistical test used to compare **variances** or assess the **overall significance** of models, especially in **hypothesis testing** involving multiple groups or variables.\n",
        "\n",
        "---\n",
        "\n",
        "### 🔍 What It Does:\n",
        "At its core, the F-test evaluates whether the **variability between groups** is significantly greater than the **variability within groups**. It uses the **F-distribution**, which is right-skewed and depends on two degrees of freedom: one for the numerator and one for the denominator.\n",
        "\n",
        "---\n",
        "\n",
        "### 🧪 Common Uses in Hypothesis Testing:\n",
        "\n",
        "1. **Comparing Two Variances**  \n",
        "   - Tests if two populations have equal variances.  \n",
        "   - **H₀**: σ₁² = σ₂²  \n",
        "   - **H₁**: σ₁² ≠ σ₂²\n",
        "\n",
        "2. **ANOVA (Analysis of Variance)**  \n",
        "   - Tests if **three or more group means** are significantly different.  \n",
        "   - The F-statistic compares **between-group variance** to **within-group variance**.\n",
        "\n",
        "3. **Regression Analysis**  \n",
        "   - Tests whether a regression model explains a significant portion of the variance in the dependent variable.  \n",
        "   - **H₀**: All regression coefficients = 0 (no effect)  \n",
        "   - **H₁**: At least one coefficient ≠ 0 (model is significant)\n",
        "\n",
        "---\n",
        "\n",
        "### 🧮 F-Statistic Formula:\n",
        "\\[\n",
        "F = \\frac{\\text{Variance between groups}}{\\text{Variance within groups}}\n",
        "\\]\n",
        "A **larger F-value** suggests that the group means are more spread out than you'd expect by chance—possibly indicating a significant effect.\n",
        "\n",
        "---\n",
        "\n",
        "### 📌 In Summary:\n",
        "The F-test is like a referee in hypothesis testing—it helps you decide whether the differences you observe are **statistically meaningful** or just random noise.\n",
        "\n"
      ],
      "metadata": {
        "id": "rndVq6pzkmuB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#<<< Practical >>>"
      ],
      "metadata": {
        "id": "96CGeiGqlATn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q1. Write a Python program to perform a Z-test for comparing a sample mean to a known population mean and interpret the results .\n",
        "Here's a Python program that performs a **one-sample Z-test** to compare a sample mean to a known population mean. It includes calculation, decision-making, and interpretation:\n",
        "\n",
        "---\n",
        "\n",
        "### 🧪 Problem Setup:\n",
        "Suppose we’re testing whether the average height of a new plant variety differs from the known population mean of **50 cm**, assuming we know the population standard deviation (σ = 5). Here's a simple dataset from our sample:\n",
        "\n"
      ],
      "metadata": {
        "id": "yq097LCnlIMK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.stats import norm\n",
        "\n",
        "# Sample data\n",
        "sample_data = [52, 49, 51, 53, 50, 48, 54, 55, 49, 51]\n",
        "population_mean = 50  # μ\n",
        "population_std = 5    # σ\n",
        "alpha = 0.05          # significance level\n",
        "\n",
        "# Step 1: Calculate sample statistics\n",
        "sample_mean = np.mean(sample_data)\n",
        "sample_size = len(sample_data)\n",
        "standard_error = population_std / np.sqrt(sample_size)\n",
        "\n",
        "# Step 2: Calculate the Z-score\n",
        "z_score = (sample_mean - population_mean) / standard_error\n",
        "\n",
        "# Step 3: Calculate the p-value (two-tailed)\n",
        "p_value = 2 * (1 - norm.cdf(abs(z_score)))\n",
        "\n",
        "# Step 4: Print results and interpret\n",
        "print(f\"Sample Mean: {sample_mean:.2f}\")\n",
        "print(f\"Z-Score: {z_score:.4f}\")\n",
        "print(f\"P-Value: {p_value:.4f}\")\n",
        "\n",
        "if p_value < alpha:\n",
        "    print(\"Result: Reject the null hypothesis. The sample mean is significantly different from the population mean.\")\n",
        "else:\n",
        "    print(\"Result: Fail to reject the null hypothesis. No significant difference found.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Izqg_5N3l7oz",
        "outputId": "3f722a48-5a5c-400b-b173-20c903ff18dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample Mean: 51.20\n",
            "Z-Score: 0.7589\n",
            "P-Value: 0.4479\n",
            "Result: Fail to reject the null hypothesis. No significant difference found.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "### 🧠 Interpretation:\n",
        "If the **p-value** is less than 0.05, we reject the null hypothesis and conclude that the sample mean is **statistically significantly different** from 50. If it’s higher, we say there’s **no significant evidence** to suggest a difference.\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "t9qcjZkommkA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q2. Simulate random data to perform hypothesis testing and calculate the corresponding P-value using Python .\n",
        "Absolutely, Sakshi! Let’s simulate a scenario where we generate random data and perform a **one-sample t-test** to see if the sample mean significantly differs from a known population mean. Here's a complete Python example:\n",
        "\n",
        "---\n",
        "\n",
        "### 🧪 Scenario:\n",
        "Suppose the average test score in a population is **75**, and we want to test if a randomly selected sample of students has a different average.\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "\n",
        "# Step 1: Simulate random sample data\n",
        "np.random.seed(42)  # for reproducibility\n",
        "sample = np.random.normal(loc=78, scale=10, size=30)  # mean=78, std=10, n=30\n",
        "\n",
        "# Step 2: Define population mean\n",
        "population_mean = 75\n",
        "\n",
        "# Step 3: Perform one-sample t-test\n",
        "t_statistic, p_value = stats.ttest_1samp(sample, population_mean)\n",
        "\n",
        "# Step 4: Print results\n",
        "print(f\"Sample Mean: {np.mean(sample):.2f}\")\n",
        "print(f\"T-Statistic: {t_statistic:.4f}\")\n",
        "print(f\"P-Value: {p_value:.4f}\")\n",
        "\n",
        "# Step 5: Interpret the result\n",
        "alpha = 0.05\n",
        "if p_value < alpha:\n",
        "    print(\"Result: Reject the null hypothesis. The sample mean is significantly different from the population mean.\")\n",
        "else:\n",
        "    print(\"Result: Fail to reject the null hypothesis. No significant difference found.\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 🧠 What’s Happening:\n",
        "- We simulate a sample of 30 students with a mean around 78.\n",
        "- We test whether this sample mean is significantly different from the population mean of 75.\n",
        "- The **p-value** tells us whether the observed difference is statistically significant.\n",
        "\n"
      ],
      "metadata": {
        "id": "WMnt1B68nLh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q3.Implement a one-sample Z-test using Python to compare the sample mean with the population mean ?\n",
        "Here's a clean Python implementation of a **one-sample Z-test** using `statsmodels`, which is perfect when the **population standard deviation is known** and the **sample size is reasonably large**:\n",
        "\n",
        "---\n",
        "\n",
        "### ✅ One-Sample Z-Test in Python\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "from statsmodels.stats.weightstats import ztest\n",
        "\n",
        "# Simulated sample data (e.g., test scores)\n",
        "sample = np.array([82, 85, 88, 90, 87, 84, 86, 89, 91, 83])\n",
        "\n",
        "# Known population mean\n",
        "population_mean = 85\n",
        "\n",
        "# Perform one-sample Z-test\n",
        "z_stat, p_value = ztest(sample, value=population_mean)\n",
        "\n",
        "# Output results\n",
        "print(f\"Sample Mean: {np.mean(sample):.2f}\")\n",
        "print(f\"Z-Statistic: {z_stat:.4f}\")\n",
        "print(f\"P-Value: {p_value:.4f}\")\n",
        "\n",
        "# Interpretation\n",
        "alpha = 0.05\n",
        "if p_value < alpha:\n",
        "    print(\"Result: Reject the null hypothesis. The sample mean is significantly different from the population mean.\")\n",
        "else:\n",
        "    print(\"Result: Fail to reject the null hypothesis. No significant difference found.\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 🧠 What’s Happening:\n",
        "- We're testing whether the sample mean differs from the population mean of 85.\n",
        "- The `ztest()` function handles the math behind the scenes.\n",
        "- The **p-value** tells us whether the difference is statistically significant."
      ],
      "metadata": {
        "id": "s-WubbghulHX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q4. Perform a two-tailed Z-test using Python and visualize the decision region on a plot .\n",
        "\n",
        "---\n",
        "\n",
        "### 🧪 Scenario:\n",
        "Suppose the population mean is 100 with a known standard deviation of 15. You collect a sample of 50 observations with a sample mean of 106. You want to test if the sample mean is significantly different from the population mean at a 5% significance level.\n",
        "\n",
        "---\n",
        "\n",
        "### ✅ Python Code with Visualization\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import norm\n",
        "\n",
        "# Parameters\n",
        "population_mean = 100\n",
        "population_std = 15\n",
        "sample_mean = 106\n",
        "sample_size = 50\n",
        "alpha = 0.05\n",
        "\n",
        "# Z-test calculation\n",
        "standard_error = population_std / np.sqrt(sample_size)\n",
        "z_score = (sample_mean - population_mean) / standard_error\n",
        "p_value = 2 * (1 - norm.cdf(abs(z_score)))\n",
        "\n",
        "# Critical z-values for two-tailed test\n",
        "z_critical = norm.ppf(1 - alpha/2)\n",
        "\n",
        "# Print results\n",
        "print(f\"Z-Score: {z_score:.4f}\")\n",
        "print(f\"P-Value: {p_value:.4f}\")\n",
        "print(f\"Z-Critical (±): ±{z_critical:.4f}\")\n",
        "\n",
        "# Visualization\n",
        "x = np.linspace(-4, 4, 1000)\n",
        "y = norm.pdf(x)\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(x, y, label='Standard Normal Distribution', color='blue')\n",
        "\n",
        "# Shade rejection regions\n",
        "plt.fill_between(x, y, where=(x <= -z_critical), color='red', alpha=0.5, label='Rejection Region (Left)')\n",
        "plt.fill_between(x, y, where=(x >= z_critical), color='red', alpha=0.5, label='Rejection Region (Right)')\n",
        "\n",
        "# Plot z-score\n",
        "plt.axvline(z_score, color='green', linestyle='--', label=f'Z-Score = {z_score:.2f}')\n",
        "\n",
        "# Labels and legend\n",
        "plt.title('Two-Tailed Z-Test Decision Regions')\n",
        "plt.xlabel('Z')\n",
        "plt.ylabel('Probability Density')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 🧠 Interpretation:\n",
        "- If the **Z-score** falls outside ±Z-critical, you **reject the null hypothesis**.\n",
        "- The shaded red areas are the **rejection regions** for a 5% significance level.\n",
        "- The green dashed line shows where your **Z-score** lands.\n",
        "\n"
      ],
      "metadata": {
        "id": "JNrkJYi4yXxJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q5. Create a Python function that calculates and visualizes Type 1 and Type 2 errors during hypothesis testing .\n",
        " Here's a Python function that simulates a hypothesis test and **visualizes Type I and Type II errors** on a normal distribution curve. This is a great way to see how significance level (α), power, and effect size interact.\n",
        "\n",
        "---\n",
        "\n",
        "### ✅ Python Function: Visualize Type I & II Errors\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import norm\n",
        "\n",
        "def visualize_type1_type2(mu_null=0, mu_alt=1, sigma=1, alpha=0.05, n=30):\n",
        "    # Standard error\n",
        "    se = sigma / np.sqrt(n)\n",
        "\n",
        "    # Critical value for right-tailed test\n",
        "    z_critical = norm.ppf(1 - alpha)\n",
        "    x_crit = mu_null + z_critical * se\n",
        "\n",
        "    # X range for plotting\n",
        "    x = np.linspace(mu_null - 4*se, mu_alt + 4*se, 1000)\n",
        "\n",
        "    # Null and alternative distributions\n",
        "    y_null = norm.pdf(x, mu_null, se)\n",
        "    y_alt = norm.pdf(x, mu_alt, se)\n",
        "\n",
        "    # Plot distributions\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(x, y_null, label='Null Hypothesis (H₀)', color='blue')\n",
        "    plt.plot(x, y_alt, label='Alternative Hypothesis (H₁)', color='green')\n",
        "\n",
        "    # Shade Type I error region (α)\n",
        "    plt.fill_between(x, y_null, where=(x >= x_crit), color='red', alpha=0.4, label='Type I Error (α)')\n",
        "\n",
        "    # Shade Type II error region (β)\n",
        "    plt.fill_between(x, y_alt, where=(x < x_crit), color='orange', alpha=0.4, label='Type II Error (β)')\n",
        "\n",
        "    # Decision boundary\n",
        "    plt.axvline(x_crit, color='black', linestyle='--', label=f'Critical Value = {x_crit:.2f}')\n",
        "\n",
        "    # Labels and legend\n",
        "    plt.title('Type I and Type II Errors in Hypothesis Testing')\n",
        "    plt.xlabel('Sample Mean')\n",
        "    plt.ylabel('Probability Density')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "# Example usage\n",
        "visualize_type1_type2(mu_null=0, mu_alt=1, sigma=1, alpha=0.05, n=30)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 🧠 What This Shows:\n",
        "- **Red area**: Type I error (rejecting H₀ when it’s true)\n",
        "- **Orange area**: Type II error (failing to reject H₀ when H₁ is true)\n",
        "- You can adjust `mu_alt`, `alpha`, or `n` to see how the errors shift\n"
      ],
      "metadata": {
        "id": "PHlVE73KzaNl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q6. Write a Python program to perform an independent T-test and interpret the results.\n",
        "\n",
        " Here's a Python program that performs an **independent two-sample t-test**—used to compare the means of two unrelated groups—and interprets the results step by step.\n",
        "\n",
        "---\n",
        "\n",
        "### 🧪 Scenario:\n",
        "Suppose we want to test whether two different teaching methods lead to different average test scores.\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "from scipy.stats import ttest_ind\n",
        "\n",
        "# Sample data: test scores from two independent groups\n",
        "group_A = np.array([85, 88, 90, 93, 87, 91, 89, 86, 90, 88])\n",
        "group_B = np.array([78, 82, 80, 79, 81, 77, 83, 80, 79, 78])\n",
        "\n",
        "# Perform independent two-sample t-test (assume equal variances)\n",
        "t_stat, p_value = ttest_ind(group_A, group_B, equal_var=True)\n",
        "\n",
        "# Output results\n",
        "print(f\"Group A Mean: {np.mean(group_A):.2f}\")\n",
        "print(f\"Group B Mean: {np.mean(group_B):.2f}\")\n",
        "print(f\"T-Statistic: {t_stat:.4f}\")\n",
        "print(f\"P-Value: {p_value:.4f}\")\n",
        "\n",
        "# Interpretation\n",
        "alpha = 0.05\n",
        "if p_value < alpha:\n",
        "    print(\"Result: Reject the null hypothesis. There is a significant difference between the group means.\")\n",
        "else:\n",
        "    print(\"Result: Fail to reject the null hypothesis. No significant difference between the group means.\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 🧠 Interpretation:\n",
        "- The **t-statistic** tells you how far apart the group means are in terms of standard error.\n",
        "- The **p-value** tells you whether that difference is statistically significant.\n",
        "- If `p < 0.05`, we conclude the teaching methods likely lead to different outcomes.\n"
      ],
      "metadata": {
        "id": "ZDIadSP20Nix"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q7. Perform a paired sample T-test using Python and visualize the comparison results\n",
        "A **paired sample t-test** is perfect when you're comparing two related sets of observations—like before-and-after measurements on the same subjects. Here's a Python program that performs the test and visualizes the results side by side.\n",
        "\n",
        "---\n",
        "\n",
        "### 🧪 Scenario:\n",
        "Let’s say we’re testing whether a training program improved test scores for 10 students.\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import ttest_rel\n",
        "\n",
        "# Sample data: scores before and after training\n",
        "before = np.array([72, 75, 78, 70, 74, 69, 73, 76, 71, 77])\n",
        "after  = np.array([78, 79, 82, 74, 77, 72, 76, 80, 75, 81])\n",
        "\n",
        "# Perform paired t-test\n",
        "t_stat, p_value = ttest_rel(before, after)\n",
        "\n",
        "# Print results\n",
        "print(f\"Mean Before: {np.mean(before):.2f}\")\n",
        "print(f\"Mean After:  {np.mean(after):.2f}\")\n",
        "print(f\"T-Statistic: {t_stat:.4f}\")\n",
        "print(f\"P-Value:     {p_value:.4f}\")\n",
        "\n",
        "# Interpretation\n",
        "alpha = 0.05\n",
        "if p_value < alpha:\n",
        "    print(\"Result: Reject the null hypothesis. The training had a significant effect.\")\n",
        "else:\n",
        "    print(\"Result: Fail to reject the null hypothesis. No significant effect detected.\")\n",
        "\n",
        "# Visualization\n",
        "x = np.arange(len(before))\n",
        "width = 0.35\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.bar(x - width/2, before, width, label='Before', color='skyblue')\n",
        "plt.bar(x + width/2, after, width, label='After', color='lightgreen')\n",
        "plt.xticks(x, [f'Student {i+1}' for i in x])\n",
        "plt.ylabel('Score')\n",
        "plt.title('Before vs After Training Scores')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 🧠 What You’ll See:\n",
        "- A **bar chart** comparing each student’s score before and after training.\n",
        "- The **t-test result** tells you if the improvement is statistically significant.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ydsWsXV91JiS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q8. Simulate data and perform both Z-test and T-test, then compare the results using Python .\n",
        " Let’s simulate a dataset and perform both a **Z-test** and a **T-test** on it to see how they compare—especially when the population standard deviation is known vs. unknown.\n",
        "\n",
        "---\n",
        "\n",
        "### 🧪 Scenario:\n",
        "We’ll simulate a sample of 40 observations from a normal distribution with a true mean of 100. We'll test whether the sample mean is significantly different from a hypothesized population mean of 105.\n",
        "\n",
        "---\n",
        "\n",
        "### ✅ Python Code\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "from statsmodels.stats.weightstats import ztest\n",
        "\n",
        "# Step 1: Simulate sample data\n",
        "np.random.seed(42)\n",
        "sample = np.random.normal(loc=100, scale=10, size=40)  # mean=100, std=10\n",
        "\n",
        "# Hypothesized population mean\n",
        "mu = 105\n",
        "alpha = 0.05\n",
        "\n",
        "# Step 2: Perform one-sample T-test (unknown population std)\n",
        "t_stat, t_p = stats.ttest_1samp(sample, mu)\n",
        "\n",
        "# Step 3: Perform one-sample Z-test (assume known population std = 10)\n",
        "z_stat, z_p = ztest(sample, value=mu, alternative='two-sided')\n",
        "\n",
        "# Step 4: Print results\n",
        "print(\"Sample Mean:\", np.mean(sample))\n",
        "print(\"\\n--- T-Test ---\")\n",
        "print(f\"T-Statistic: {t_stat:.4f}\")\n",
        "print(f\"P-Value:     {t_p:.4f}\")\n",
        "\n",
        "print(\"\\n--- Z-Test ---\")\n",
        "print(f\"Z-Statistic: {z_stat:.4f}\")\n",
        "print(f\"P-Value:     {z_p:.4f}\")\n",
        "\n",
        "# Step 5: Interpretation\n",
        "print(\"\\n--- Interpretation ---\")\n",
        "if t_p < alpha:\n",
        "    print(\"T-Test: Reject the null hypothesis.\")\n",
        "else:\n",
        "    print(\"T-Test: Fail to reject the null hypothesis.\")\n",
        "\n",
        "if z_p < alpha:\n",
        "    print(\"Z-Test: Reject the null hypothesis.\")\n",
        "else:\n",
        "    print(\"Z-Test: Fail to reject the null hypothesis.\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 🧠 Key Takeaways:\n",
        "- The **T-test** uses the **sample standard deviation**, making it more conservative for small samples.\n",
        "- The **Z-test** assumes the **population standard deviation is known**, which can lead to slightly different results.\n",
        "- As sample size increases, both tests tend to converge.\n"
      ],
      "metadata": {
        "id": "fP3_FbKU2Bpz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q9. Write a Python function to calculate the confidence interval for a sample mean and explain its significance.\n",
        " Here's a Python function that calculates the **confidence interval** for a sample mean using the **t-distribution**, which is ideal when the population standard deviation is unknown and the sample size is small.\n",
        "\n",
        "---\n",
        "\n",
        "### ✅ Python Function: Confidence Interval for Sample Mean\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "\n",
        "def confidence_interval(data, confidence=0.95):\n",
        "    \"\"\"\n",
        "    Calculate the confidence interval for a sample mean.\n",
        "\n",
        "    Parameters:\n",
        "        data (list or array): Sample data\n",
        "        confidence (float): Confidence level (default is 0.95)\n",
        "\n",
        "    Returns:\n",
        "        tuple: (mean, lower bound, upper bound)\n",
        "    \"\"\"\n",
        "    data = np.array(data)\n",
        "    n = len(data)\n",
        "    mean = np.mean(data)\n",
        "    sem = stats.sem(data)  # Standard error of the mean\n",
        "    margin = sem * stats.t.ppf((1 + confidence) / 2, df=n-1)\n",
        "    return mean, mean - margin, mean + margin\n",
        "\n",
        "# Example usage\n",
        "sample = [12, 15, 14, 10, 13, 17, 14, 15, 16, 14]\n",
        "mean, lower, upper = confidence_interval(sample)\n",
        "print(f\"Sample Mean: {mean:.2f}\")\n",
        "print(f\"95% Confidence Interval: ({lower:.2f}, {upper:.2f})\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 🧠 Why It Matters:\n",
        "A **confidence interval** gives you a range of plausible values for the population mean based on your sample. For example, a 95% confidence interval means that if you repeated the sampling process many times, about 95% of those intervals would contain the true population mean.\n",
        "\n",
        "It’s a powerful way to express **uncertainty** and **reliability** in your estimates—especially useful in research, surveys, and A/B testing.\n",
        "\n"
      ],
      "metadata": {
        "id": "iTj0KhAa2gk4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q10. Write a Python program to calculate the margin of error for a given confidence level using sample data .\n",
        "\n",
        " Here's a Python program that calculates the **margin of error** for a sample mean using the **t-distribution**—perfect when the population standard deviation is unknown:\n",
        "\n",
        "---\n",
        "\n",
        "### ✅ Python Program: Margin of Error for a Given Confidence Level\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "\n",
        "def margin_of_error(data, confidence=0.95):\n",
        "    \"\"\"\n",
        "    Calculate the margin of error for a sample mean using the t-distribution.\n",
        "\n",
        "    Parameters:\n",
        "        data (list or array): Sample data\n",
        "        confidence (float): Confidence level (default is 0.95)\n",
        "\n",
        "    Returns:\n",
        "        float: Margin of error\n",
        "    \"\"\"\n",
        "    data = np.array(data)\n",
        "    n = len(data)\n",
        "    sem = stats.sem(data)  # Standard error of the mean\n",
        "    t_critical = stats.t.ppf((1 + confidence) / 2, df=n-1)\n",
        "    moe = t_critical * sem\n",
        "    return moe\n",
        "\n",
        "# Example usage\n",
        "sample = [12, 15, 14, 10, 13, 17, 14, 15, 16, 14]\n",
        "moe = margin_of_error(sample, confidence=0.95)\n",
        "print(f\"Margin of Error (95% confidence): ±{moe:.2f}\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 🧠 Why It Matters:\n",
        "The **margin of error** tells you how much your sample estimate might vary from the true population value. It’s a key part of building **confidence intervals** and understanding the **precision** of your results.\n",
        "\n"
      ],
      "metadata": {
        "id": "7HAjHlCB3sD2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q11. Implement a Bayesian inference method using Bayes' Theorem in Python and explain the process.\n",
        "\n",
        "---\n",
        "\n",
        "### 🧠 Step-by-Step: Bayes’ Theorem Refresher\n",
        "\n",
        "\\[\n",
        "P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)}\n",
        "\\]\n",
        "\n",
        "Where:\n",
        "- **P(A)** = Prior probability (e.g. having the disease)\n",
        "- **P(B|A)** = Likelihood (e.g. testing positive if diseased)\n",
        "- **P(B)** = Marginal probability of testing positive\n",
        "- **P(A|B)** = Posterior probability (updated belief)\n",
        "\n",
        "---\n",
        "\n",
        "### 🧪 Python Implementation\n",
        "\n",
        "```python\n",
        "def bayesian_inference(p_disease, p_pos_given_disease, p_pos_given_no_disease):\n",
        "    p_no_disease = 1 - p_disease\n",
        "    p_pos = (p_pos_given_disease * p_disease) + (p_pos_given_no_disease * p_no_disease)\n",
        "    p_disease_given_pos = (p_pos_given_disease * p_disease) / p_pos\n",
        "    return p_disease_given_pos\n",
        "\n",
        "# Example values\n",
        "p_disease = 0.01                  # 1% of population has the disease\n",
        "p_pos_given_disease = 0.99        # 99% sensitivity\n",
        "p_pos_given_no_disease = 0.05     # 5% false positive rate\n",
        "\n",
        "posterior = bayesian_inference(p_disease, p_pos_given_disease, p_pos_given_no_disease)\n",
        "print(f\"Probability of having the disease given a positive test: {posterior:.4f}\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 🔍 Interpretation:\n",
        "Even with a highly accurate test, the **posterior probability** might be surprisingly low if the disease is rare. This is the power of Bayesian thinking—it forces us to consider **base rates** and not just test accuracy.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "Y_2ncB0s4MkH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q12. Perform a Chi-square test for independence between two categorical variables in Python .\n",
        "\n",
        "---\n",
        "\n",
        "### 🧪 Scenario:\n",
        "Suppose we surveyed 100 people about their **preferred beverage** (Tea or Coffee) and their **work shift** (Day or Night). Here's the observed data:\n",
        "\n",
        "|            | Tea | Coffee |\n",
        "|------------|-----|--------|\n",
        "| Day Shift  | 20  | 30     |\n",
        "| Night Shift| 25  | 25     |\n",
        "\n",
        "---\n",
        "\n",
        "### ✅ Python Code\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "from scipy.stats import chi2_contingency\n",
        "\n",
        "# Step 1: Create the contingency table\n",
        "data = np.array([[20, 30],\n",
        "                 [25, 25]])\n",
        "\n",
        "# Step 2: Perform the Chi-square test\n",
        "chi2, p, dof, expected = chi2_contingency(data)\n",
        "\n",
        "# Step 3: Display results\n",
        "print(f\"Chi-square Statistic: {chi2:.4f}\")\n",
        "print(f\"P-value: {p:.4f}\")\n",
        "print(f\"Degrees of Freedom: {dof}\")\n",
        "print(\"Expected Frequencies:\\n\", expected)\n",
        "\n",
        "# Step 4: Interpret the result\n",
        "alpha = 0.05\n",
        "if p < alpha:\n",
        "    print(\"Result: Reject the null hypothesis. The variables are dependent.\")\n",
        "else:\n",
        "    print(\"Result: Fail to reject the null hypothesis. The variables are independent.\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 🧠 Interpretation:\n",
        "- **Null hypothesis (H₀)**: Beverage preference is independent of work shift.\n",
        "- **Alternative hypothesis (H₁)**: Beverage preference depends on work shift.\n",
        "- If the **p-value < 0.05**, we conclude there’s a significant association.\n",
        "\n"
      ],
      "metadata": {
        "id": "ncD57qa27CLG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q13. Write a Python program to calculate the expected frequencies for a Chi-square test based on observed data .\n",
        "**bold text** Here's a Python program that calculates the **expected frequencies** for a Chi-square test using a contingency table of observed values. This is a key step in performing a **Chi-square test for independence**.\n",
        "\n",
        "---\n",
        "\n",
        "### ✅ Python Program: Calculate Expected Frequencies\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "from scipy.stats import chi2_contingency\n",
        "\n",
        "# Step 1: Define the observed frequency table\n",
        "# Example: Beverage preference by work shift\n",
        "observed = np.array([[20, 30],\n",
        "                     [25, 25]])\n",
        "\n",
        "# Step 2: Use chi2_contingency to get expected frequencies\n",
        "chi2, p, dof, expected = chi2_contingency(observed)\n",
        "\n",
        "# Step 3: Display results\n",
        "print(\"Observed Frequencies:\\n\", observed)\n",
        "print(\"\\nExpected Frequencies:\\n\", expected.round(2))\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 🧠 How It Works:\n",
        "- `chi2_contingency()` automatically computes the expected frequencies under the assumption that the two variables are independent.\n",
        "- The expected frequency for each cell is calculated as:\n",
        "  \\[\n",
        "  E_{ij} = \\frac{(\\text{Row Total}_i) \\times (\\text{Column Total}_j)}{\\text{Grand Total}}\n",
        "  \\]\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "ixJndXWc7uxM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q14. Perform a goodness-of-fit test using Python to compare the observed data to an expected distribution .\n",
        " Let’s perform a **Chi-square goodness-of-fit test** in Python to compare observed data to an expected distribution. This test helps determine whether your observed frequencies differ significantly from what you'd expect under a theoretical model.\n",
        "\n",
        "---\n",
        "\n",
        "### 🧪 Example Scenario:\n",
        "Suppose you roll a 6-sided die 60 times and get the following observed counts:\n",
        "\n",
        "```python\n",
        "observed = [8, 9, 10, 11, 12, 10]  # Frequencies for faces 1–6\n",
        "```\n",
        "\n",
        "If the die is fair, you'd expect each face to appear 10 times:\n",
        "\n",
        "```python\n",
        "expected = [10, 10, 10, 10, 10, 10]\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### ✅ Python Code\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "from scipy.stats import chisquare\n",
        "\n",
        "# Observed and expected frequencies\n",
        "observed = np.array([8, 9, 10, 11, 12, 10])\n",
        "expected = np.array([10, 10, 10, 10, 10, 10])\n",
        "\n",
        "# Perform Chi-square goodness-of-fit test\n",
        "chi2_stat, p_value = chisquare(f_obs=observed, f_exp=expected)\n",
        "\n",
        "# Output results\n",
        "print(f\"Chi-square Statistic: {chi2_stat:.4f}\")\n",
        "print(f\"P-value: {p_value:.4f}\")\n",
        "\n",
        "# Interpretation\n",
        "alpha = 0.05\n",
        "if p_value < alpha:\n",
        "    print(\"Result: Reject the null hypothesis. The observed distribution differs significantly from the expected.\")\n",
        "else:\n",
        "    print(\"Result: Fail to reject the null hypothesis. No significant difference found.\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 🧠 Why It Matters:\n",
        "This test is perfect for checking if your data fits a **uniform**, **binomial**, or any other expected distribution. It’s widely used in quality control, genetics, and survey analysis.\n"
      ],
      "metadata": {
        "id": "9whJptdN8RHU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q15. Create a Python script to simulate and visualize the Chi-square distribution and discuss its characteristics .\n",
        " Here's a Python script that simulates and visualizes the **Chi-square distribution** using `scipy.stats` and `matplotlib`. We'll also explore how its shape changes with different degrees of freedom (df).\n",
        "\n",
        "---\n",
        "\n",
        "### ✅ Python Script: Simulate & Visualize Chi-square Distribution\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import chi2\n",
        "\n",
        "# Degrees of freedom to visualize\n",
        "dfs = [1, 2, 5, 10, 20]\n",
        "x = np.linspace(0, 40, 1000)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "for df in dfs:\n",
        "    plt.plot(x, chi2.pdf(x, df), label=f'df = {df}')\n",
        "\n",
        "# Plot formatting\n",
        "plt.title('Chi-square Distribution for Various Degrees of Freedom')\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('Probability Density')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 📊 Characteristics of the Chi-square Distribution:\n",
        "\n",
        "- **Non-negative**: Values are always ≥ 0 since it's based on squared values.\n",
        "- **Right-skewed**: Especially for small degrees of freedom.\n",
        "- **Asymptotic**: The tail extends infinitely to the right.\n",
        "- **Shape depends on degrees of freedom**:\n",
        "  - For **df = 1 or 2**, the distribution is highly skewed.\n",
        "  - As **df increases**, it becomes more symmetric and approaches a normal distribution.\n",
        "- **Applications**:\n",
        "  - **Goodness-of-fit tests**\n",
        "  - **Tests of independence** in contingency tables\n",
        "  - **Tests for population variance**\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "zb9nAJ6G9szs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q16. Implement an F-test using Python to compare the variances of two random samples .\n",
        " Here's a Python program that performs an **F-test** to compare the variances of two independent samples. This is useful when you want to test if two populations have **equal variability**.\n",
        "\n",
        "---\n",
        "\n",
        "### 🧪 Scenario:\n",
        "Let’s say we have two groups of measurements from different machines, and we want to test if their output variances are significantly different.\n",
        "\n",
        "---\n",
        "\n",
        "### ✅ Python Code: F-Test for Equality of Variances\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "from scipy.stats import f\n",
        "\n",
        "# Step 1: Simulate two random samples\n",
        "np.random.seed(42)\n",
        "sample1 = np.random.normal(loc=50, scale=5, size=30)   # mean=50, std=5\n",
        "sample2 = np.random.normal(loc=52, scale=8, size=30)   # mean=52, std=8\n",
        "\n",
        "# Step 2: Calculate sample variances\n",
        "var1 = np.var(sample1, ddof=1)\n",
        "var2 = np.var(sample2, ddof=1)\n",
        "\n",
        "# Step 3: Compute F-statistic (larger variance / smaller variance)\n",
        "if var1 > var2:\n",
        "    F = var1 / var2\n",
        "    dfn, dfd = len(sample1) - 1, len(sample2) - 1\n",
        "else:\n",
        "    F = var2 / var1\n",
        "    dfn, dfd = len(sample2) - 1, len(sample1) - 1\n",
        "\n",
        "# Step 4: Calculate p-value (two-tailed)\n",
        "p_value = 2 * min(f.cdf(F, dfn, dfd), 1 - f.cdf(F, dfn, dfd))\n",
        "\n",
        "# Step 5: Output results\n",
        "print(f\"Variance 1: {var1:.2f}\")\n",
        "print(f\"Variance 2: {var2:.2f}\")\n",
        "print(f\"F-Statistic: {F:.4f}\")\n",
        "print(f\"P-Value: {p_value:.4f}\")\n",
        "\n",
        "# Step 6: Interpretation\n",
        "alpha = 0.05\n",
        "if p_value < alpha:\n",
        "    print(\"Result: Reject the null hypothesis. Variances are significantly different.\")\n",
        "else:\n",
        "    print(\"Result: Fail to reject the null hypothesis. No significant difference in variances.\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 🧠 What’s Happening:\n",
        "- The **F-statistic** is the ratio of the two sample variances.\n",
        "- The **p-value** tells us whether the difference is statistically significant.\n",
        "- We use a **two-tailed test** to detect inequality in either direction.\n",
        "\n"
      ],
      "metadata": {
        "id": "8M97EUQL-PiH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q17. Write a Python program to perform an ANOVA test to compare means between multiple groups and interpret the results .\n",
        "Here's a Python program that performs a **one-way ANOVA (Analysis of Variance)** to test whether the means of three or more independent groups are significantly different.\n",
        "\n",
        "---\n",
        "\n",
        "### 🧪 Scenario:\n",
        "Suppose we’re comparing test scores from students taught using three different teaching methods.\n",
        "\n",
        "---\n",
        "\n",
        "### ✅ Python Code: One-Way ANOVA\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "from scipy.stats import f_oneway\n",
        "\n",
        "# Sample data: test scores from three teaching methods\n",
        "method_A = [85, 88, 90, 87, 86]\n",
        "method_B = [78, 82, 80, 79, 81]\n",
        "method_C = [92, 94, 91, 93, 95]\n",
        "\n",
        "# Perform one-way ANOVA\n",
        "f_stat, p_value = f_oneway(method_A, method_B, method_C)\n",
        "\n",
        "# Output results\n",
        "print(f\"F-Statistic: {f_stat:.4f}\")\n",
        "print(f\"P-Value: {p_value:.4f}\")\n",
        "\n",
        "# Interpretation\n",
        "alpha = 0.05\n",
        "if p_value < alpha:\n",
        "    print(\"Result: Reject the null hypothesis. At least one group mean is significantly different.\")\n",
        "else:\n",
        "    print(\"Result: Fail to reject the null hypothesis. No significant difference between group means.\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 🧠 Interpretation:\n",
        "- **Null hypothesis (H₀)**: All group means are equal.\n",
        "- **Alternative hypothesis (H₁)**: At least one group mean is different.\n",
        "- If the **p-value < 0.05**, we conclude that **teaching method affects performance**.\n",
        "\n"
      ],
      "metadata": {
        "id": "-_aQXhp3-uSP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q18. Perform a one-way ANOVA test using Python to compare the means of different groups and plot the results .\n",
        " Here's a Python program that performs a **one-way ANOVA test** and visualizes the group means using a boxplot—perfect for comparing multiple groups at once.\n",
        "\n",
        "---\n",
        "\n",
        "### 🧪 Scenario:\n",
        "Suppose we’re comparing exam scores from students taught using three different teaching methods.\n",
        "\n",
        "---\n",
        "\n",
        "### ✅ Python Code: One-Way ANOVA with Visualization\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import f_oneway\n",
        "\n",
        "# Sample data: scores from three teaching methods\n",
        "method_A = [85, 88, 90, 87, 86]\n",
        "method_B = [78, 82, 80, 79, 81]\n",
        "method_C = [92, 94, 91, 93, 95]\n",
        "\n",
        "# Perform one-way ANOVA\n",
        "f_stat, p_value = f_oneway(method_A, method_B, method_C)\n",
        "\n",
        "# Print results\n",
        "print(f\"F-Statistic: {f_stat:.4f}\")\n",
        "print(f\"P-Value: {p_value:.4f}\")\n",
        "\n",
        "# Interpretation\n",
        "alpha = 0.05\n",
        "if p_value < alpha:\n",
        "    print(\"Result: Reject the null hypothesis. At least one group mean is significantly different.\")\n",
        "else:\n",
        "    print(\"Result: Fail to reject the null hypothesis. No significant difference between group means.\")\n",
        "\n",
        "# Visualization\n",
        "data = [method_A, method_B, method_C]\n",
        "labels = ['Method A', 'Method B', 'Method C']\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.boxplot(data, labels=labels, patch_artist=True)\n",
        "plt.title('Comparison of Teaching Methods')\n",
        "plt.ylabel('Test Scores')\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 🧠 What This Tells You:\n",
        "- The **F-statistic** measures the ratio of between-group to within-group variance.\n",
        "- A **low p-value** (typically < 0.05) suggests that **at least one group mean differs** significantly.\n",
        "- The **boxplot** helps you visually compare the spread and central tendency of each group.\n",
        "\n"
      ],
      "metadata": {
        "id": "g2YOGqur_OeO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q19. Write a Python function to check the assumptions (normality, independence, and equal variance) for ANOVAD .\n",
        " Before running an ANOVA, it's essential to check its assumptions: **normality**, **independence**, and **homogeneity of variances**. Here's a Python function that helps assess these using statistical tests and visualizations:\n",
        "\n",
        "---\n",
        "\n",
        "### ✅ Python Function to Check ANOVA Assumptions\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy.stats import shapiro, levene\n",
        "from statsmodels.graphics.gofplots import qqplot\n",
        "\n",
        "def check_anova_assumptions(data, group_col, value_col):\n",
        "    \"\"\"\n",
        "    Check ANOVA assumptions: normality, independence (visually), and equal variance.\n",
        "\n",
        "    Parameters:\n",
        "        data (DataFrame): Input dataset\n",
        "        group_col (str): Column name for group labels\n",
        "        value_col (str): Column name for numeric values\n",
        "    \"\"\"\n",
        "    groups = data[group_col].unique()\n",
        "    print(\"🔍 Checking Normality (Shapiro-Wilk Test):\")\n",
        "    for group in groups:\n",
        "        vals = data[data[group_col] == group][value_col]\n",
        "        stat, p = shapiro(vals)\n",
        "        print(f\"  {group}: W={stat:.4f}, p={p:.4f} → {'Normal' if p > 0.05 else 'Not normal'}\")\n",
        "\n",
        "    print(\"\\n📊 Checking Homogeneity of Variance (Levene’s Test):\")\n",
        "    samples = [data[data[group_col] == g][value_col] for g in groups]\n",
        "    stat, p = levene(*samples)\n",
        "    print(f\"  Levene’s W={stat:.4f}, p={p:.4f} → {'Equal variances' if p > 0.05 else 'Unequal variances'}\")\n",
        "\n",
        "    print(\"\\n👁️ Visual Check for Independence and Normality:\")\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    # Boxplot for spread\n",
        "    plt.subplot(1, 2, 1)\n",
        "    sns.boxplot(x=group_col, y=value_col, data=data)\n",
        "    plt.title(\"Boxplot by Group\")\n",
        "\n",
        "    # Q-Q plot of residuals\n",
        "    plt.subplot(1, 2, 2)\n",
        "    model_resid = data[value_col] - data.groupby(group_col)[value_col].transform('mean')\n",
        "    qqplot(model_resid, line='s', ax=plt.gca())\n",
        "    plt.title(\"Q-Q Plot of Residuals\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 🧠 What It Checks:\n",
        "- **Normality**: Shapiro-Wilk test for each group\n",
        "- **Equal variance**: Levene’s test across groups\n",
        "- **Independence**: Visual check via boxplots and Q-Q plot of residuals\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "urHMC9BC_t6T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q20. Perform a two-way ANOVA test using Python to study the interaction between two factors and visualize the results.\n",
        " Let’s perform a **two-way ANOVA** in Python to analyze the effects of two categorical factors on a continuous outcome—and visualize the interaction.\n",
        "\n",
        "---\n",
        "\n",
        "### 🧪 Scenario:\n",
        "Suppose we’re studying how **teaching method** (`Method A`, `Method B`) and **study time** (`Short`, `Long`) affect **test scores**.\n",
        "\n",
        "---\n",
        "\n",
        "### ✅ Python Code: Two-Way ANOVA with Visualization\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.formula.api import ols\n",
        "\n",
        "# Step 1: Create sample data\n",
        "data = pd.DataFrame({\n",
        "    'Method': np.repeat(['A', 'B'], 10),\n",
        "    'Time': np.tile(np.repeat(['Short', 'Long'], 5), 2),\n",
        "    'Score': [70, 72, 68, 71, 69, 75, 78, 76, 77, 74,\n",
        "              65, 67, 66, 68, 64, 80, 82, 81, 83, 79]\n",
        "})\n",
        "\n",
        "# Step 2: Fit the two-way ANOVA model\n",
        "model = ols('Score ~ C(Method) + C(Time) + C(Method):C(Time)', data=data).fit()\n",
        "anova_table = sm.stats.anova_lm(model, typ=2)\n",
        "print(anova_table)\n",
        "\n",
        "# Step 3: Visualize interaction\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.pointplot(data=data, x='Time', y='Score', hue='Method', capsize=0.1, dodge=True, markers='o')\n",
        "plt.title('Interaction Plot: Method × Time')\n",
        "plt.ylabel('Mean Test Score')\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 🧠 Interpretation:\n",
        "- The **ANOVA table** shows p-values for:\n",
        "  - Main effects: `Method`, `Time`\n",
        "  - **Interaction**: `Method × Time`\n",
        "- If the interaction p-value is **< 0.05**, it suggests the effect of one factor depends on the level of the other.\n",
        "- The **interaction plot** helps visualize whether the lines cross or diverge—indicating interaction.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "4X0Ln1H7AMIc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q21. Write a Python program to visualize the F-distribution and discuss its use in hypothesis testing .\n",
        "\n",
        " Here's a Python program that visualizes the **F-distribution** for different degrees of freedom and explains how it's used in hypothesis testing—especially in **ANOVA** and **variance comparison**.\n",
        "\n",
        "---\n",
        "\n",
        "### ✅ Python Code: Visualize F-Distribution\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import f\n",
        "\n",
        "# Define degrees of freedom for numerator and denominator\n",
        "df_pairs = [(1, 10), (5, 10), (10, 20), (20, 30)]\n",
        "x = np.linspace(0, 5, 1000)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "for dfn, dfd in df_pairs:\n",
        "    y = f.pdf(x, dfn, dfd)\n",
        "    plt.plot(x, y, label=f'df1={dfn}, df2={dfd}')\n",
        "\n",
        "plt.title('F-Distribution for Various Degrees of Freedom')\n",
        "plt.xlabel('F-value')\n",
        "plt.ylabel('Probability Density')\n",
        "plt.legend()\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 🧠 Why the F-Distribution Matters in Hypothesis Testing:\n",
        "\n",
        "- **Used in ANOVA**: To test if **three or more group means** are significantly different.\n",
        "- **Used in variance comparison**: To test if **two populations have equal variances**.\n",
        "- **F-statistic**: Ratio of two scaled variances. A large F-value suggests a significant difference between groups or variances.\n",
        "- **Right-skewed**: The distribution is always non-negative and skewed right, especially with small degrees of freedom.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "u-5Yg0IhAtMY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q22. Perform a one-way ANOVA test in Python and visualize the results with boxplots to compare group means .\n",
        " Here's a complete Python example that performs a **one-way ANOVA** and visualizes the group means using **boxplots**—a great way to compare distributions across categories.\n",
        "\n",
        "---\n",
        "\n",
        "### 🧪 Scenario:\n",
        "Suppose we’re comparing exam scores from students taught using three different teaching methods.\n",
        "\n",
        "---\n",
        "\n",
        "### ✅ Python Code: One-Way ANOVA + Boxplot\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from scipy.stats import f_oneway\n",
        "\n",
        "# Step 1: Create sample data\n",
        "data = {\n",
        "    'Score': [85, 88, 90, 87, 86, 78, 82, 80, 79, 81, 92, 94, 91, 93, 95],\n",
        "    'Method': ['A']*5 + ['B']*5 + ['C']*5\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Step 2: Perform one-way ANOVA\n",
        "grouped = [df[df['Method'] == m]['Score'] for m in df['Method'].unique()]\n",
        "f_stat, p_value = f_oneway(*grouped)\n",
        "\n",
        "# Step 3: Print results\n",
        "print(f\"F-Statistic: {f_stat:.4f}\")\n",
        "print(f\"P-Value: {p_value:.4f}\")\n",
        "alpha = 0.05\n",
        "if p_value < alpha:\n",
        "    print(\"Result: Reject the null hypothesis. At least one group mean is significantly different.\")\n",
        "else:\n",
        "    print(\"Result: Fail to reject the null hypothesis. No significant difference between group means.\")\n",
        "\n",
        "# Step 4: Visualize with boxplot\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.boxplot(x='Method', y='Score', data=df, palette='Set2')\n",
        "plt.title('Comparison of Test Scores by Teaching Method')\n",
        "plt.ylabel('Score')\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 🧠 Interpretation:\n",
        "- The **F-statistic** tells you how much the group means differ relative to within-group variability.\n",
        "- The **p-value** helps you decide whether the observed differences are statistically significant.\n",
        "- The **boxplot** visually compares the spread and central tendency of each group.\n",
        "\n"
      ],
      "metadata": {
        "id": "E3Qa385WBJkH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q23. Simulate random data from a normal distribution, then perform hypothesis testing to evaluate the means .\n",
        "Let’s simulate data from a **normal distribution**, then perform a **one-sample t-test** to evaluate whether the sample mean significantly differs from a hypothesized population mean.\n",
        "\n",
        "---\n",
        "\n",
        "### ✅ Python Code: Simulate & Test the Mean\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "\n",
        "# Step 1: Simulate random data\n",
        "np.random.seed(42)\n",
        "sample = np.random.normal(loc=102, scale=10, size=50)  # mean=102, std=10, n=50\n",
        "\n",
        "# Step 2: Define population mean to test against\n",
        "mu = 100  # hypothesized population mean\n",
        "\n",
        "# Step 3: Perform one-sample t-test\n",
        "t_stat, p_value = stats.ttest_1samp(sample, mu)\n",
        "\n",
        "# Step 4: Output results\n",
        "print(f\"Sample Mean: {np.mean(sample):.2f}\")\n",
        "print(f\"T-Statistic: {t_stat:.4f}\")\n",
        "print(f\"P-Value: {p_value:.4f}\")\n",
        "\n",
        "# Step 5: Interpretation\n",
        "alpha = 0.05\n",
        "if p_value < alpha:\n",
        "    print(\"Result: Reject the null hypothesis. The sample mean is significantly different from the population mean.\")\n",
        "else:\n",
        "    print(\"Result: Fail to reject the null hypothesis. No significant difference found.\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 🧠 What’s Happening:\n",
        "- We simulate 50 values from a normal distribution with a true mean of 102.\n",
        "- We test whether this sample could plausibly come from a population with mean 100.\n",
        "- The **p-value** tells us whether the observed difference is statistically significant.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "XiLP-dpiBr52"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q24. Perform a hypothesis test for population variance using a Chi-square distribution and interpret the results .\n",
        "Let’s walk through how to perform a **hypothesis test for population variance** using the **Chi-square distribution** in Python. This test helps determine whether the variance of a population equals a specified value.\n",
        "\n",
        "---\n",
        "\n",
        "### 🧪 Scenario:\n",
        "Suppose a machine is designed to produce bolts with a variance in length of **4 mm²**. We collect a sample of 25 bolts and want to test if the actual variance differs from this target.\n",
        "\n",
        "---\n",
        "\n",
        "### ✅ Python Code: Chi-square Test for Variance\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "from scipy.stats import chi2\n",
        "\n",
        "# Step 1: Sample data\n",
        "np.random.seed(42)\n",
        "sample = np.random.normal(loc=50, scale=2.5, size=25)  # std = √6.25 = 2.5 → variance = 6.25\n",
        "\n",
        "# Step 2: Hypothesized population variance\n",
        "sigma_squared_0 = 4  # claimed variance\n",
        "\n",
        "# Step 3: Calculate test statistic\n",
        "n = len(sample)\n",
        "sample_variance = np.var(sample, ddof=1)\n",
        "chi2_stat = (n - 1) * sample_variance / sigma_squared_0\n",
        "\n",
        "# Step 4: Compute p-value (two-tailed)\n",
        "alpha = 0.05\n",
        "df = n - 1\n",
        "p_lower = chi2.cdf(chi2_stat, df)\n",
        "p_upper = 1 - chi2.cdf(chi2_stat, df)\n",
        "p_value = 2 * min(p_lower, p_upper)\n",
        "\n",
        "# Step 5: Output results\n",
        "print(f\"Sample Variance: {sample_variance:.4f}\")\n",
        "print(f\"Chi-square Statistic: {chi2_stat:.4f}\")\n",
        "print(f\"P-Value: {p_value:.4f}\")\n",
        "\n",
        "# Step 6: Interpretation\n",
        "if p_value < alpha:\n",
        "    print(\"Result: Reject the null hypothesis. The population variance is significantly different from the hypothesized value.\")\n",
        "else:\n",
        "    print(\"Result: Fail to reject the null hypothesis. No significant difference in variance.\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 🧠 Interpretation:\n",
        "- **Null Hypothesis (H₀)**: Population variance = 4 mm²\n",
        "- **Alternative Hypothesis (H₁)**: Population variance ≠ 4 mm²\n",
        "- The **Chi-square statistic** measures how far the sample variance deviates from the hypothesized variance.\n",
        "- A **small p-value** indicates that the observed variance is unlikely under H₀."
      ],
      "metadata": {
        "id": "onyANKG1COR-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q25. Write a Python script to perform a Z-test for comparing proportions between two datasets or groups .\n",
        "Here's a Python script that performs a **two-proportion Z-test**—perfect for comparing success rates between two independent groups, like conversion rates in A/B testing or survey responses.\n",
        "\n",
        "---\n",
        "\n",
        "### ✅ Python Script: Two-Proportion Z-Test\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "from statsmodels.stats.proportion import proportions_ztest\n",
        "\n",
        "# Step 1: Define the number of successes and observations in each group\n",
        "# Example: Group A had 60 successes out of 150 trials, Group B had 80 out of 200\n",
        "successes = np.array([60, 80])\n",
        "samples = np.array([150, 200])\n",
        "\n",
        "# Step 2: Perform the two-proportion Z-test\n",
        "z_stat, p_value = proportions_ztest(count=successes, nobs=samples, alternative='two-sided')\n",
        "\n",
        "# Step 3: Output results\n",
        "print(f\"Z-Statistic: {z_stat:.4f}\")\n",
        "print(f\"P-Value: {p_value:.4f}\")\n",
        "\n",
        "# Step 4: Interpretation\n",
        "alpha = 0.05\n",
        "if p_value < alpha:\n",
        "    print(\"Result: Reject the null hypothesis. The proportions are significantly different.\")\n",
        "else:\n",
        "    print(\"Result: Fail to reject the null hypothesis. No significant difference in proportions.\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 🧠 When to Use This:\n",
        "- Comparing **click-through rates** between two ads\n",
        "- Evaluating **pass rates** between two classes\n",
        "- Testing **conversion rates** in A/B experiments\n",
        "\n"
      ],
      "metadata": {
        "id": "AcfkUt90C3UD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q26. Implement an F-test for comparing the variances of two datasets, then interpret and visualize the results .\n",
        "Let’s implement an **F-test** to compare the variances of two datasets, interpret the result, and visualize the distributions along with the F-statistic.\n",
        "\n",
        "---\n",
        "\n",
        "### ✅ Python Code: F-Test with Visualization\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import f\n",
        "\n",
        "# Step 1: Simulate two datasets\n",
        "np.random.seed(42)\n",
        "data1 = np.random.normal(loc=50, scale=5, size=30)   # std = 5\n",
        "data2 = np.random.normal(loc=52, scale=8, size=30)   # std = 8\n",
        "\n",
        "# Step 2: Calculate sample variances\n",
        "var1 = np.var(data1, ddof=1)\n",
        "var2 = np.var(data2, ddof=1)\n",
        "\n",
        "# Step 3: Compute F-statistic (larger variance / smaller variance)\n",
        "if var1 > var2:\n",
        "    F = var1 / var2\n",
        "    dfn, dfd = len(data1) - 1, len(data2) - 1\n",
        "else:\n",
        "    F = var2 / var1\n",
        "    dfn, dfd = len(data2) - 1, len(data1) - 1\n",
        "\n",
        "# Step 4: Compute p-value (two-tailed)\n",
        "p_value = 2 * min(f.cdf(F, dfn, dfd), 1 - f.cdf(F, dfn, dfd))\n",
        "\n",
        "# Step 5: Print results\n",
        "print(f\"Variance 1: {var1:.2f}\")\n",
        "print(f\"Variance 2: {var2:.2f}\")\n",
        "print(f\"F-Statistic: {F:.4f}\")\n",
        "print(f\"P-Value: {p_value:.4f}\")\n",
        "\n",
        "# Step 6: Interpretation\n",
        "alpha = 0.05\n",
        "if p_value < alpha:\n",
        "    print(\"Result: Reject the null hypothesis. Variances are significantly different.\")\n",
        "else:\n",
        "    print(\"Result: Fail to reject the null hypothesis. No significant difference in variances.\")\n",
        "\n",
        "# Step 7: Visualization\n",
        "x = np.linspace(0, 5, 1000)\n",
        "y = f.pdf(x, dfn, dfd)\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(x, y, label=f'F-distribution (df1={dfn}, df2={dfd})', color='blue')\n",
        "plt.axvline(F, color='red', linestyle='--', label=f'F-statistic = {F:.2f}')\n",
        "plt.fill_between(x, y, where=(x >= F), color='red', alpha=0.3, label='Rejection Region')\n",
        "plt.title('F-Test: Comparing Variances')\n",
        "plt.xlabel('F-value')\n",
        "plt.ylabel('Probability Density')\n",
        "plt.legend()\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 🧠 Interpretation:\n",
        "- The **F-statistic** is the ratio of the two sample variances.\n",
        "- The **p-value** tells us whether the observed difference in variances is statistically significant.\n",
        "- The **plot** shows the F-distribution and where the test statistic falls—highlighting the rejection region.\n"
      ],
      "metadata": {
        "id": "RI32x9rYDNDJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q27. Perform a Chi-square test for goodness of fit with simulated data and analyze the results.\n",
        " Let’s simulate some categorical data and perform a **Chi-square goodness-of-fit test** to see if the observed frequencies match an expected distribution.\n",
        "\n",
        "---\n",
        "\n",
        "### 🧪 Scenario:\n",
        "Suppose we roll a 6-sided die 120 times and get the following observed counts:\n",
        "\n",
        "```python\n",
        "observed = [15, 22, 18, 20, 25, 20]  # Frequencies for faces 1–6\n",
        "```\n",
        "\n",
        "If the die is fair, we expect each face to appear 20 times:\n",
        "\n",
        "```python\n",
        "expected = [20, 20, 20, 20, 20, 20]\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### ✅ Python Code: Chi-square Goodness-of-Fit Test\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "from scipy.stats import chisquare\n",
        "\n",
        "# Observed and expected frequencies\n",
        "observed = np.array([15, 22, 18, 20, 25, 20])\n",
        "expected = np.array([20] * 6)\n",
        "\n",
        "# Perform Chi-square test\n",
        "chi2_stat, p_value = chisquare(f_obs=observed, f_exp=expected)\n",
        "\n",
        "# Output results\n",
        "print(f\"Chi-square Statistic: {chi2_stat:.4f}\")\n",
        "print(f\"P-Value: {p_value:.4f}\")\n",
        "\n",
        "# Interpretation\n",
        "alpha = 0.05\n",
        "if p_value < alpha:\n",
        "    print(\"Result: Reject the null hypothesis. The die may not be fair.\")\n",
        "else:\n",
        "    print(\"Result: Fail to reject the null hypothesis. No significant evidence the die is unfair.\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 🧠 Interpretation:\n",
        "- **Null Hypothesis (H₀)**: The die is fair (uniform distribution).\n",
        "- **Alternative Hypothesis (H₁)**: The die is not fair.\n",
        "- If the **p-value < 0.05**, we conclude the observed distribution differs significantly from the expected.\n",
        "\n"
      ],
      "metadata": {
        "id": "lCo0m8W5Djrk"
      }
    }
  ]
}